<!DOCTYPE html>
<html lang="en" >

<head>
  <meta charset="UTF-8">
  <title>Inteligencia artificial</title>
  <link href="https://fonts.googleapis.com/css?family=Quicksand:400,500,700" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/5.0.0/normalize.min.css">

  
      <link rel="stylesheet" href="css/style.css">

  
</head>

<body>

  <h1>Historia de la inteligencia artificial, <span>1943–Presente</span></h1>
<div class="flex-parent">
	<div class="input-flex-container">
		<div class="input active">
			<span data-year="1910-1955" data-info="Génesis de la IA"></span>
		</div>
		<div class="input">
			<span data-year="1956" data-info="Nacimiento de la IA"></span>
		</div>
		<div class="input">
			<span data-year="1952-1969" data-info="Entusiasmo inicial, grandes esperanzas"></span>
		</div>
		<div class="input">
			<span data-year="1966-1973" data-info="Una dosis de realidad"></span>
		</div>
		<div class="input">
			<span data-year="1969-1979" data-info="Sistemas basados en el conocimiento"></span>
		</div>
		<div class="input">
			<span data-year="1980-Presente" data-info="La IA se convierte en una industria"></span>
		</div>
		<div class="input">
			<span data-year="1986-Presente" data-info="Regreso de las redes neuronales"></span>
		</div>
		<div class="input">
			<span data-year="1987-Presente" data-info="La IA se convierte en una ciencia"></span>
		</div>
		<div class="input">
			<span data-year="1995-Presente" data-info="Emergencia de los sistemas inteligentes"></span>
		</div>
	</div>
	<div class="description-flex-container">
		<p class="active">Warren McCulloch y Walter Pitts (1943) han sido reconocidos como los autores del primer trabajo de IA. Partieron de tres fuentes: conocimientos sobre la fisiología básica y funcionamiento de las neuronas en el cerebro, el análisis formal de la lógica proposicional de Russell  y Whitehead y la teoría de la computación de Turing. Propusieron un modelo constituido por neuronas artificiales, en el que cada una de ellas se caracterizan por estar (activada) o (desactivada); la (activación) se daba como respuesta a la estimulación producida por una cantidad suficiente de neuronas vecinas. El estado de una neurona se veía como (equivalente, de hecho, a uno proposición con unos estímulos adecuados). Mostraron, por ejemplo, que cualquier función de cómputo podría calcularse mediante alguna red de neuronas interconectadas, y que todos los conectores lógicos (and, or, not, etc.) se podrían implementar utilizando estructuras de red sencillas. McCulloch y Pitts también sugirieron que redes adecuadamente definidas podrían aprender. Donald Hebb (1949) propuso y demostró una sencilla regla de actualización para modificar las intensidades de las conexiones entre neuronas. Su regla, ahora llamada de aprendizaje Hebbiano o de Hebb, sigue vigente en la actualidad.</p>
		<p>Princeton acogió a otras de las figuras señeras de la IA. John McCarthy. Posteriormente a su graduación, McCarthy se transladó al Dartmouth college, que se erigiría en lugar del nacimiento oficial de este campo. McCarthy convenció a Minsky, Claude Shannon y Nathaniel Rochester para que le ayudaran a aumentar el interés de los investigadores americanos en la teoría de autómatas, las redes neuronales y el estudio de la inteligencia. Organizaron un taller con una duración de dos meses en Darmouth en el verano de 1956. Hubo diez asistentes en total, entre los que se incluían Trenchard More de Princeton, Arthur Samuel de IBM, y Ray Solomonoff y Oliver Sclifridge del MIT.
			Dos investigadores del Carnegie Tech, Allen Newell y Herbert Simon acapararon
			la atención. Si bien los demás también tenían algunas ideas y, en algunos casos, programas
			para aplicaciones deterininadas como el juego de damas. Newell y Simon contaban
			ya con un programa de razonamiento, el Teórico Lógico (TL), del que Simon
			afirmaba: Hemos inventado un programa de computación capaz de pensar de manera
			no numérica. con lo que ha quedado resuelto el venerable problema de la dualidad mente-cuerpo.
		</p>
		<p>Los primeros años de la IA estuvieron llenos de éxitos (aunque con ciertas limitaciones). Teniendo en cuenta lo primitvo de los computadores y las herramientas de programación de aquella época, y el hecho de que sólo unos pocos años antes, a los computadores se les consideraba como artefactos que podían realizar trabajos aritméticos y nada más, resultó sorprendente que un computador hiciese algo remotamente inteligente. La comunidad científica, en su mayoría, prefirió creer que una máquina nunca podría hacer tareas. Naturalmente, los investigadores de IA responderían demostrando la realización de una tarea tras otra. John McCarthy se refiere a esta época como la era de: Mira mamá, ahora sin manos.
			En IBM, Nathaniel Rochester y sus colegas desarrollaron algunos de los primeros programas de IA. Herbert Gelernter (1959) construyó el demostrador de teoremas de gemotría (DTG), el cual era capaz de probar teoremas que muchos estudiantes de matemáticas podían encontrar muy complejos de resolver. A comienzo de 1962. ARthur Samuel escribió una serie de programas para el juego de damas que eventualmente aprendieron a juegar hasta alcanzar un nivel equivalente de un amateur. De paso, echó por tierra la idea de que los computadores sólo pueden hacer lo que se les dice: su programa pronto aprendió a jugar mejor que su creador. El programa se presentó en televisión en febrero de 1956 y causó una gran impresión. Como Turing, Samuel tenía dificultades para obtener el tiempo de cómputo. Trabajaba por las noches y utilizaba máquinas que aún estaban en periodo de prueba en la planta de fabricación de IBM.
		</p>
		<p>Herbert Simon 1957: “Sin afán de sorprenderlos y dejarlos atónitos, pero la forma más sencilla que tengo de resumirlo es diciéndoles que actualmente en el mundo existen máquinas capaces de pensar, aprender y crear. Además. Su aptitud para hacer lo anterior aumentará rápidamente hasta que (en un futuro previsible) la magnitud de problemas que serán capaces de resolver irá a la par que la capacidad de la mente humana para hacer lo mismo”.

			En un informe presentado en 1966, el comité consultivo declaró que “no se ha logrado obtener ninguna traducción de textos científicos generales ni se prevé obtener ninguna en un futuro inmediato”. Hoy en día, la traducción automática es una herramienta imperfecta, pero de uso extendido en documentos técnicos, comerciales, gubernamentales y de internet.
			
			Otro problema fue que muchos de los problemas que se estaban intentando resolver mediante IA eran intratables. La incapacidad para manejar la “explosión combinatoria” fue una de las principales críticas que se hicieron a la IA, razón por la cual el gobierno británico retiro la ayuda a las investigaciones de IA.</p>
		<p>El lingüista transformado en informático Roger Schank afirmó “No Existe eso que llaman sintaxis”, lo que irrito a muchos lingüistas, pero sirvió para iniciar un útil debate.
			Schank y sus estudiantes diseñaron una serie de programas (Schan y Abelson, 1977; Wilensky,1978, Schank y Riesbeck, 1981; Dyer, 1983) cuyo objetivo era la comprensión del lenguaje natural. El foco de atención estaba más en los problemas vinculados a la representación y razonamiento del conocimiento necesario para la comprensión del lenguaje. 
			El crecimiento generalizado de aplicaciones para solucionar problemas del mundo real provoco el respectivo aumento en la demanda de esquemas de representación del conocimiento que funcionaran. Se desarrolló una considerable cantidad de lenguajes de representación y razonamiento diferentes. Algunos basados en lógica.</p>
		<p>El primer sistema experto comercial que tuvo éxito, RI, inicio su actividad en Digital Equipment Corporation(McDermott,  1982). El programa se utilizaba en la elaboración de pedidos de nuevos sistemas informáticos. En 1986 representaba para la compañía un ahorro estimado de 40 millones de dólares al año. En 1988 casi todas las compañías importantes de Estados Unidos contaban con su propio grupo de IA, en el que se utilizaban o investigaban sistemas expertos.

			En 1981 los japoneses anunciaron el proyecto “Quinta Generación”, un plan de diez años para construir computadoras inteligentes en los que pudiese ejecutarse Prolog.
			Como respuesta Estados unidos constituyo la Microelectronics and Computer Technology Corporation (MCC), consorcio encargado de mantener la competitividad nacional en estas áreas.
			
			En su conjunto, la industria de la IA crecio rápidamente, pasando de unos pocos millones de dólares en 1980 a billones de dólares en 1988. Poco después de este periodo llego la época llamada “El invierno de la IA”, que afecto a muchas empresas que no fueron capaces de desarrollar los extravagantes productos prometidos.
			</p>
		<p>Psicólogos como David Rumelhart y Geoff Hinton continuaron con el estudio de modelos de memoria basados en redes neuronales. El impulso más fuerte se produjo a mediados de la década de los 80, cuando por lo menos cuatro grupos distintos reinventaron el algoritmo de aprendizaje de retroalimentación, mencionado por primera vez en 1969 por Bryson y Ho. 
			El algoritmo se aplicó a diversos problemas de aprendizaje en los campos de la informática y la psicología, y la gran difusión que conocieron los resultados obtenidos, publicados en la colección Paralled Distributed Processing (Rumelhart y McClelland, 1986).
			Aquellos modelos de inteligencia artificial llamados conexionistas fueron vistos por algunos como competidores tanto del os modelos simbólicos propuestos por Newell y Simon como de la aproximación lógica de McCarthy entre otros (Smolensky, 1988).</p>
		<p>La IA se fundó en parte en el marco de una rebelión en contra de las limitaciones de los campos existentes como la teoría de control o la estadística, y ahora abarca estos campos. Tal y como indica David McAllester(1998), En los primeros años de la IA parecía perfectamente posible que las nuevas formas de la computación simbólica, por ejemplo, los marcos y las redes semánticas, hicieran que la mayor parte de la teoría clásica pasara a ser obsoleta.
			En términos metodológicas, se puede decir, con rotundidad, que la IA ya forma parte del ámbito de los métodos científicos. Para que se acepten, las hipótesis se deben someter a rigurosos experimentos empíricos, y los resultados deben analizarse estadísticamente para identificar su relevancia (Cohen, 1995).
			Un buen modelo de la tendencia actual es el campo del reconocimiento del habla. 
			La aparición de Probabilistic Reasoning in Intelligent Systems de Judea Pearl (1988) hizo que se aceptara de nuevo la probabilidad y la teoría de la decisión como parte de la IA, como consecuencia del resurgimiento del interés despertado y gracias especialmente al artículo In Defense of Probability de Peter Cheeseman (1985).
			El formalismo de las redes de Bayes aprecio para facilitar la representación eficiente y el razonamiento riguroso en situaciones en las que se disponía de conocimiento incierto. Este enfoque supera con creces muchos de los problemas de los sistemas de razonamiento probabilístico de las décadas de los 60 y 70; y ahora domina la investigación de la IA en el razonamiento incierto y los sistemas expertos.
			El trabajo de Judea Pearl (1982) y de Eric Horvitz y David Heckerman (Horvitz y Heckerman, 1986) sirvió para promover la noción de sistemas expertos normativos, es decir, los que actúan racionalmente de acuerdo con las leyes de la teoría de la decisión, sin que intenten imitar las etapas de razonamiento de los expertos humanos.</p>
			<p>
				Por el progreso en la solución de subproblemas de IA, los investigadores han comenzado a trabajar de nuevo en el problema del “agente total”. El trabajo de Allen Newell, John Laird y Paul Rosenbloom en SOAR (Newell, 1990; Laird, 1987) es el ejemplo mejor conocido de una arquitectura de agente completa.
Uno de los medios más importantes para los agentes inteligentes es Internet. Los sistemas de IA han llegado a ser tan comunes en aplicaciones desarrolladas para la Web que el sufijo “bot” se ha introducido en el lenguaje común. Más aun, tecnologías de IA son la base de muchas herramientas para Internet, como por ejemplo motores de búsqueda, sistemas de recomendación, y los sistemas para la construcción de portales Web.
Ademas de la priemra edición de este libro de texto (Russell y Norving, 1995), otros libros de texto han adoptado recientemente la perspectiva de agentes (Poole, 1998; Nilsson, 1998). Una de las conclusiones que se han extraído al tratar de construir agentes completos ha sido que se deberían reorganizar los sub campos aislados de la IA para que sus resultados se puedan interrelacionar. 
			</p>
	</div>
</div>



  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js'></script>

  

    <script  src="js/index.js"></script>




</body>

</html>
